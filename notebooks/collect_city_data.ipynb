{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: F401\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import django\n",
    "from django.conf import settings  # pyright: ignore[reportUnusedImport]\n",
    "\n",
    "# import altair as alt\n",
    "import polars as pl\n",
    "import yaml\n",
    "\n",
    "# from great_tables import GT\n",
    "\n",
    "# Allow Django to run in async environments (like Jupyter)\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "# Set the Django settings module\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'paths.settings')\n",
    "\n",
    "# Configure Django\n",
    "django.setup()\n",
    "\n",
    "from common import polars as ppl  # noqa: E402\n",
    "from nodes.constants import FORECAST_COLUMN, VALUE_COLUMN, YEAR_COLUMN  # noqa: E402\n",
    "from nodes.exceptions import NodeComputationError  # noqa: E402\n",
    "from nodes.units import Quantity, unit_registry  # noqa: E402\n",
    "from notebooks.notebook_support import get_context, get_nodes, initialize_notebook_env  # noqa: E402\n",
    "\n",
    "initialize_notebook_env()\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from common.polars import PathsDataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c722cf8f",
   "metadata": {},
   "source": [
    "There was a previous version of this functionality. It was organized as a Django management command. It had the benefit that it was accessing the production instances directly, but otherwise it was very cumbersome code.\n",
    "https://github.com/kausaltech/kausal-paths/blob/4fd525e23bb96cefdd991a10b3354daf9fe3be3c/nodes/management/commands/collect_city_data.py\n",
    "\n",
    "This code version may be better as a terminal command, but I'll keep the Jupyter notebook version for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365377ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../netzeroplanner-framework-config/emission_potential.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NodeData:\n",
    "    \"\"\"Individual node with its dataframe.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    df: ppl.PathsDataFrame\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InstanceData:\n",
    "    \"\"\"Instance containing multiple nodes.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    target_year: int\n",
    "    nodes: list[NodeData] = field(default_factory=list)\n",
    "\n",
    "    def add_node(self, node_id: str, df: ppl.PathsDataFrame) -> NodeData:\n",
    "        \"\"\"Add a node to this instance.\"\"\"\n",
    "\n",
    "        node = NodeData(id=node_id, df=df)\n",
    "        self.nodes.append(node)\n",
    "        return node\n",
    "\n",
    "    def get_node_df(self, node_id: str) -> ppl.PathsDataFrame | None:\n",
    "        \"\"\"Get a specific node df by id.\"\"\"\n",
    "\n",
    "        node = next((node for node in self.nodes if node.id == node_id), None)\n",
    "        if node is None:\n",
    "            return None\n",
    "        return node.df\n",
    "\n",
    "    def update_node_df(self, node_id: str, df: ppl.PathsDataFrame) -> InstanceData:\n",
    "        node = next((node for node in self.nodes if node.id == node_id), None)\n",
    "        assert node is not None\n",
    "        node.df = df\n",
    "        return self\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollection:\n",
    "    \"\"\"Main container for all dc.\"\"\"\n",
    "\n",
    "    output_path: str\n",
    "    output_date: str\n",
    "    processors: list[str] = field(default_factory=list)\n",
    "    logs: list[str] = field(default_factory=list)\n",
    "    instances: list[InstanceData] = field(default_factory=list)\n",
    "    summaries: list[InstanceData] = field(default_factory=list)\n",
    "\n",
    "    def add_instance(self, instance_id: str, target_year: int) -> InstanceData:\n",
    "        \"\"\"Add a new instance.\"\"\"\n",
    "        instance = InstanceData(id=instance_id, target_year=target_year)\n",
    "        self.instances.append(instance)\n",
    "        return instance\n",
    "\n",
    "    def get_instance(self, instance_id: str) -> InstanceData | None:\n",
    "        \"\"\"Get a specific instance by id.\"\"\"\n",
    "\n",
    "        return next((inst for inst in self.instances if inst.id == instance_id), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(yaml_file):\n",
    "    config = yaml.safe_load(Path(yaml_file).open('r'))  # noqa: SIM115\n",
    "    return config\n",
    "\n",
    "def find_target_values(dc: DataCollection) -> DataCollection:\n",
    "    for instance in dc.instances:\n",
    "        for node in instance.nodes:\n",
    "            df: ppl.PathsDataFrame = node.df\n",
    "            meta = df.get_meta()\n",
    "            target_year = instance.target_year\n",
    "            obs_year = df.filter(~pl.col(FORECAST_COLUMN))[YEAR_COLUMN].max()\n",
    "            df = (\n",
    "                df.filter(pl.col(YEAR_COLUMN).is_in([obs_year, target_year]))\n",
    "                .sort(by=[YEAR_COLUMN])\n",
    "            )\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(YEAR_COLUMN) == obs_year)\n",
    "                .then(pl.lit('newest'))\n",
    "                .otherwise(pl.lit('target'))\n",
    "                .alias('param')\n",
    "            )\n",
    "            df = ppl.to_ppdf(df, meta).add_to_index('param')\n",
    "            instance.update_node_df(node.id, df)\n",
    "    return dc\n",
    "\n",
    "def convert_to_target_units(dc: DataCollection) -> DataCollection:\n",
    "    multipliers: dict[str, Quantity] = {\n",
    "        'kt_co2e/a': unit_registry('1 * kt/kt_co2e'),\n",
    "    }\n",
    "    for instance in dc.instances:\n",
    "        for node in instance.nodes:\n",
    "            df: PathsDataFrame = node.df\n",
    "            df_unit = df.get_meta().units[VALUE_COLUMN]\n",
    "            for from_unit, to_unit in multipliers.items():\n",
    "                if df_unit.is_compatible_with(from_unit):\n",
    "                    df = df.multiply_quantity(VALUE_COLUMN, to_unit)\n",
    "                    instance.update_node_df(node.id, df)\n",
    "    return dc\n",
    "\n",
    "def sum_over_dfs(dc: DataCollection) -> DataCollection:\n",
    "    for instance in dc.instances:\n",
    "        for node in instance.nodes:\n",
    "            df = node.df\n",
    "            dropcols = [dim for dim in df.primary_keys if dim != YEAR_COLUMN]\n",
    "            df = df.paths.sum_over_dims(dropcols)\n",
    "            instance.update_node_df(node_id=node.id, df=df)\n",
    "    return dc\n",
    "\n",
    "def sum_over_instances(dc: DataCollection) -> DataCollection:\n",
    "    # node_ids = list({node.id for instance in dc.instances for node in instance.nodes})\n",
    "    summary = InstanceData(id='sum_over_instances', target_year=0)\n",
    "    for instance in dc.instances:\n",
    "        for node in instance.nodes:\n",
    "            df: PathsDataFrame = node.df\n",
    "            sum_df: PathsDataFrame | None = summary.get_node_df(node.id)\n",
    "            if sum_df is None:\n",
    "                summary.add_node(node.id, df)\n",
    "            elif set(sum_df.primary_keys) == set(df.primary_keys):\n",
    "                summary.update_node_df(node.id, sum_df.paths.add_df(odf=df))\n",
    "            else:\n",
    "                dc.logs.append(\"\".join([\n",
    "                    f\"Node {node.id} has primary keys {df.primary_keys} in instance {instance.id}\",\n",
    "                    f\" but expected {sum_df.primary_keys}.\"]))\n",
    "    dc.summaries.append(summary)\n",
    "\n",
    "    return dc\n",
    "\n",
    "def report_log(dc: DataCollection) -> None:\n",
    "    print(\"\\nDuring processing, the following things happened:\")\n",
    "    for log in dc.logs:\n",
    "        print(log)\n",
    "\n",
    "def save_summaries(dc: DataCollection) -> DataCollection:\n",
    "    dc.logs.append(\"Saving summaries about:\")\n",
    "    output_path = dc.output_path\n",
    "    for summary in dc.summaries:\n",
    "        dc.logs.append(f\"- {summary.id}.\")\n",
    "        for node in summary.nodes:\n",
    "            output_file = f\"{output_path}{summary.id}_{node.id}.csv\"\n",
    "            node.df.write_csv(output_file)\n",
    "            dc.logs.append(f\"  - Saved dc in {output_file}.\")\n",
    "    return dc\n",
    "\n",
    "def no_processing(dc: DataCollection) -> DataCollection:\n",
    "    return dc\n",
    "\n",
    "postprocess_data = {\n",
    "    'convert_to_target_units': convert_to_target_units,\n",
    "    'find_target_values': find_target_values,\n",
    "    'save_summaries': save_summaries,\n",
    "    'sum_over_dfs': sum_over_dfs,\n",
    "    'sum_over_instances': sum_over_instances,\n",
    "    'none': no_processing,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e95627",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(config_file)\n",
    "processors = config.get('processors')\n",
    "output_path = config.get('output_path')\n",
    "output_date: str = str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))  # noqa: DTZ005\n",
    "\n",
    "data = DataCollection(output_path, output_date, processors=processors)\n",
    "\n",
    "instances = config['instances']\n",
    "# instances = instances[0:10] # Used to simplify testing\n",
    "node_ids = [node['id'] for node in config['nodes']]\n",
    "\n",
    "for instance_id in instances:\n",
    "    nodes = get_nodes(instance_id)\n",
    "    target_year = get_context(instance_id).target_year\n",
    "    instance = data.add_instance(instance_id=instance_id, target_year=target_year)\n",
    "    for node_id in node_ids:\n",
    "        node = nodes.get(node_id)\n",
    "        if node is None:\n",
    "            data.logs.append(f\"Node {node_id} not found in instance {instance.id}.\")\n",
    "            continue\n",
    "        try:\n",
    "            df = node.get_output_pl()\n",
    "            instance.add_node(node_id=node_id, df=df)\n",
    "        except (ValueError, NodeComputationError):\n",
    "            data.logs.append(f\"Node {node_id} in instance {instance.id} gave and error and is skipped.\")\n",
    "            continue\n",
    "\n",
    "for processor in data.processors:\n",
    "    if processor not in postprocess_data.keys():\n",
    "        data.logs.append(f\"Processor {processor} is not defined. Ignoring.\")\n",
    "        continue\n",
    "    data = postprocess_data[processor](data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_log(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
