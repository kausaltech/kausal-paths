{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: F401\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import django\n",
    "from django.conf import settings  # pyright: ignore[reportUnusedImport]\n",
    "\n",
    "# import altair as alt\n",
    "import polars as pl\n",
    "import yaml\n",
    "\n",
    "# from great_tables import GT\n",
    "\n",
    "# Allow Django to run in async environments (like Jupyter)\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "# Set the Django settings module\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'paths.settings')\n",
    "\n",
    "# Configure Django\n",
    "django.setup()\n",
    "\n",
    "from common import polars as ppl  # noqa: E402\n",
    "from nodes.constants import FORECAST_COLUMN, VALUE_COLUMN, YEAR_COLUMN  # noqa: E402\n",
    "from nodes.exceptions import NodeComputationError  # noqa: E402\n",
    "from nodes.units import Quantity, unit_registry  # noqa: E402\n",
    "from notebooks.notebook_support import get_context, get_nodes, initialize_notebook_env  # noqa: E402\n",
    "\n",
    "initialize_notebook_env()\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from common.polars import PathsDataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c722cf8f",
   "metadata": {},
   "source": [
    "There was a previous version of this functionality. It was organized as a Django management command. It had the benefit that it was accessing the production instances directly, but otherwise it was very cumbersome code.\n",
    "https://github.com/kausaltech/kausal-paths/blob/4fd525e23bb96cefdd991a10b3354daf9fe3be3c/nodes/management/commands/collect_city_data.py\n",
    "\n",
    "This code version may be better as a terminal command, but I'll keep the Jupyter notebook version for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365377ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../netzeroplanner-framework-config/emission_potential.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NodeData:\n",
    "    \"\"\"Individual node with its dataframe.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    df: ppl.PathsDataFrame\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InstanceData:\n",
    "    \"\"\"Instance containing multiple nodes.\"\"\"\n",
    "\n",
    "    id: str\n",
    "    target_year: int\n",
    "    nodes: list[NodeData] = field(default_factory=list)\n",
    "\n",
    "    def add_node(self, node_id: str, df: ppl.PathsDataFrame) -> NodeData:\n",
    "        \"\"\"Add a node to this instance.\"\"\"\n",
    "\n",
    "        node = NodeData(id=node_id, df=df)\n",
    "        self.nodes.append(node)\n",
    "        return node\n",
    "\n",
    "    def get_node_df(self, node_id: str) -> ppl.PathsDataFrame | None:\n",
    "        \"\"\"Get a specific node df by id.\"\"\"\n",
    "\n",
    "        node = next((node for node in self.nodes if node.id == node_id), None)\n",
    "        if node is None:\n",
    "            return None\n",
    "        return node.df\n",
    "\n",
    "    def update_node_df(self, node_id: str, df: ppl.PathsDataFrame) -> InstanceData:\n",
    "        node = next((node for node in self.nodes if node.id == node_id), None)\n",
    "        assert node is not None\n",
    "        node.df = df\n",
    "        return self\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollection:\n",
    "    \"\"\"Main container for all dc.\"\"\"\n",
    "\n",
    "    output_path: str\n",
    "    output_date: str\n",
    "    processors: list[str]\n",
    "    logs: list[str]\n",
    "    instances: list[InstanceData]\n",
    "    summaries: list[InstanceData]\n",
    "    target_units: dict[str, str]\n",
    "\n",
    "    def add_instance(self, instance_id: str, target_year: int) -> InstanceData:\n",
    "        \"\"\"Add a new instance.\"\"\"\n",
    "        instance = InstanceData(id=instance_id, target_year=target_year)\n",
    "        self.instances.append(instance)\n",
    "        return instance\n",
    "\n",
    "    def get_instance(self, instance_id: str) -> InstanceData | None:\n",
    "        \"\"\"Get a specific instance by id.\"\"\"\n",
    "\n",
    "        return next((inst for inst in self.instances if inst.id == instance_id), None)\n",
    "\n",
    "    def read_config(self, yaml_file):\n",
    "        config = yaml.safe_load(Path(yaml_file).open('r'))  # noqa: SIM115\n",
    "        return config\n",
    "\n",
    "    def find_target_values(self) -> DataCollection:\n",
    "        for instance in self.instances:\n",
    "            for node in instance.nodes:\n",
    "                df: ppl.PathsDataFrame = node.df\n",
    "                meta = df.get_meta()\n",
    "                target_year = instance.target_year\n",
    "                obs_year = df.filter(~pl.col(FORECAST_COLUMN))[YEAR_COLUMN].max()\n",
    "                df = (\n",
    "                    df.filter(pl.col(YEAR_COLUMN).is_in([obs_year, target_year]))\n",
    "                    .sort(by=[YEAR_COLUMN])\n",
    "                )\n",
    "                df = df.with_columns(\n",
    "                    pl.when(pl.col(YEAR_COLUMN) == obs_year)\n",
    "                    .then(pl.lit('newest'))\n",
    "                    .otherwise(pl.lit('target'))\n",
    "                    .alias('param')\n",
    "                )\n",
    "                df = ppl.to_ppdf(df, meta).add_to_index('param')\n",
    "                instance.update_node_df(node.id, df)\n",
    "        return self\n",
    "\n",
    "    def convert_to_target_units(self) -> DataCollection:\n",
    "        multipliers: dict[str, Quantity] = {\n",
    "            'kt_co2e/a': unit_registry('1 * kt/kt_co2e'),\n",
    "        }\n",
    "        for instance in self.instances:\n",
    "            for node in instance.nodes:\n",
    "                df: PathsDataFrame = node.df\n",
    "                df_unit = df.get_meta().units[VALUE_COLUMN]\n",
    "                for from_unit, to_unit in multipliers.items():\n",
    "                    if df_unit.is_compatible_with(from_unit):\n",
    "                        df = df.multiply_quantity(VALUE_COLUMN, to_unit)\n",
    "                df = df.ensure_unit(VALUE_COLUMN, self.target_units[node.id])\n",
    "                instance.update_node_df(node.id, df)\n",
    "        return self\n",
    "\n",
    "    def sum_over_dims(self) -> DataCollection:\n",
    "        for instance in self.instances:\n",
    "            for node in instance.nodes:\n",
    "                df = node.df\n",
    "                dropcols = [dim for dim in df.primary_keys if dim != YEAR_COLUMN]\n",
    "                df = df.paths.sum_over_dims(dropcols)\n",
    "                instance.update_node_df(node_id=node.id, df=df)\n",
    "        return self\n",
    "\n",
    "    def sum_over_instances(self) -> DataCollection:\n",
    "        # node_ids = list({node.id for instance in dc.instances for node in instance.nodes})\n",
    "        summary = InstanceData(id='sum_over_instances', target_year=0)\n",
    "        for instance in self.instances:\n",
    "            for node in instance.nodes:\n",
    "                df: PathsDataFrame = node.df\n",
    "                df = (df\n",
    "                    .with_columns(pl.lit(instance.id).alias('Instance'))\n",
    "                    .add_to_index('Instance')\n",
    "                )\n",
    "                sum_df: PathsDataFrame | None = summary.get_node_df(node.id)\n",
    "                if sum_df is None:\n",
    "                    summary.add_node(node.id, df)\n",
    "                elif set(sum_df.primary_keys) == set(df.primary_keys):\n",
    "                    summary.update_node_df(node.id, sum_df.paths.concat_vertical(df))\n",
    "                else:\n",
    "                    print(df.head())\n",
    "                    print(sum_df.head())\n",
    "                    self.logs.append(\"\".join([\n",
    "                        f\"Node {node.id} has primary keys {df.primary_keys} in instance {instance.id}\",\n",
    "                        f\" but expected {sum_df.primary_keys}. Ignore the node in sum.\"]))\n",
    "        for node in summary.nodes:\n",
    "            total = node.df.paths.sum_over_dims(['Instance', YEAR_COLUMN])\n",
    "            total = total.with_columns([\n",
    "                pl.lit('TOTAL').alias('Instance'),\n",
    "                pl.lit(0).alias(YEAR_COLUMN)]).add_to_index(['Instance', YEAR_COLUMN])\n",
    "            assert set(node.df.columns) == set(total.columns)\n",
    "            total = total.select(node.df.columns)\n",
    "            summary.update_node_df(node.id, node.df.paths.concat_vertical(total))\n",
    "        self.summaries.append(summary)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def report_log(self) -> None:\n",
    "        print(\"\\nDuring processing, the following things happened:\")\n",
    "        for log in self.logs:\n",
    "            print(log)\n",
    "\n",
    "    def save_summaries(self) -> DataCollection:\n",
    "        self.logs.append(\"Saving summaries about:\")\n",
    "        output_path = self.output_path\n",
    "        for summary in self.summaries:\n",
    "            self.logs.append(f\"- {summary.id}.\")\n",
    "            for node in summary.nodes:\n",
    "                output_file = f\"{output_path}{summary.id}_{node.id}.csv\"\n",
    "                node.df.write_csv(output_file)\n",
    "                self.logs.append(f\"  - Saved nodes {node.id} in {output_file}.\")\n",
    "        return self\n",
    "\n",
    "    def no_processing(self) -> DataCollection:\n",
    "        return self\n",
    "\n",
    "    def __init__(self, config_file: str):\n",
    "        config = self.read_config(config_file)\n",
    "        processors = config.get('processors')\n",
    "        output_path = config.get('output_path')\n",
    "        output_date: str = str(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))  # noqa: DTZ005\n",
    "\n",
    "        self.output_path = output_path\n",
    "        self.output_date = output_date\n",
    "        self.processors = processors\n",
    "        self.instances = []\n",
    "        self.summaries = []\n",
    "        self.target_units = {node['id']: node['target_unit'] for node in config['nodes'] }\n",
    "        self.logs = [f\"Collect data from {config_file}.\"]\n",
    "\n",
    "        instances = config['instances']\n",
    "        instances = instances[50:60] # Used to simplify testing\n",
    "        node_ids = [node['id'] for node in config['nodes']]\n",
    "\n",
    "        for instance_id in instances:\n",
    "            try:\n",
    "                context = get_context(instance_id)\n",
    "            except FileNotFoundError:\n",
    "                self.logs.append(f\"Instance {instance_id} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            nodes = get_nodes(instance_id)\n",
    "            target_year = context.target_year\n",
    "            instance = self.add_instance(instance_id=instance_id, target_year=target_year)\n",
    "            for node_id in node_ids:\n",
    "                node = nodes.get(node_id)\n",
    "                if node is None:\n",
    "                    self.logs.append(f\"Node {node_id} not found in instance {instance.id}.\")\n",
    "                    continue\n",
    "                try:\n",
    "                    df = node.get_output_pl()\n",
    "                    instance.add_node(node_id=node_id, df=df)\n",
    "                except (ValueError, NodeComputationError):\n",
    "                    self.logs.append(f\"Node {node_id} in instance {instance.id} gave and error and is skipped.\")\n",
    "                    continue\n",
    "\n",
    "    def process_data(self) -> DataCollection:\n",
    "\n",
    "        PROCESS_DATA = {\n",
    "            'convert_to_target_units': self.convert_to_target_units,\n",
    "            'find_target_values': self.find_target_values,\n",
    "            'save_summaries': self.save_summaries,\n",
    "            'sum_over_dims': self.sum_over_dims,\n",
    "            'sum_over_instances': self.sum_over_instances,\n",
    "            'none': self.no_processing,\n",
    "        }\n",
    "        dc = self\n",
    "        for processor in dc.processors:\n",
    "            if processor not in PROCESS_DATA.keys():\n",
    "                dc.logs.append(f\"Processor {processor} is not defined. Ignoring.\")\n",
    "                continue\n",
    "            dc.logs.append(f\"Processing {processor} ...\")\n",
    "            dc = PROCESS_DATA[processor]()\n",
    "        return dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DataCollection(config_file=config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e95627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.process_data()\n",
    "dc.report_log()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
